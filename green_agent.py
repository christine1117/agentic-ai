"""
PersonaGym Mental Health Green Agent (AgentBeats Compatible)
Team: EmpaTeam
"""

import json
import os
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime


from agentbeats import Agent

import anthropic
from openai import OpenAI


class PersonaGymGreenAgent(Agent):
    """
    Green agent that orchestrates PersonaGym evaluations for mental health scenarios.
    """
    
    def __init__(self, **kwargs):
   
        super().__init__(**kwargs)
        
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        try:
            self.anthropic_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        except:
            print("Warning: ANTHROPIC_API_KEY not found. Claude evaluation might fail.")
            self.anthropic_client = None
        
      
        self.personas = [
            "Licensed Clinical Therapist specialized in CBT",
            "Supportive Grandmother with life experience",
            "School Counselor working with adolescents",
            "Crisis Hotline Operator trained in de-escalation",
            "Life Coach focused on personal development",
            "Peer Support Specialist in recovery",
            "Compassionate Friend offering emotional support"
        ]
        
        self.environments = [
            "Therapy Session",
            "Crisis Intervention",
            "Family Dinner Conversation",
            "School Counseling Office",
            "Support Group Meeting",
            "Late Night Phone Call",
            "Online Chat Support"
        ]
        
        self.dimensions = [
            "expected_action", "action_justification", "linguistic_habits",
            "persona_consistency", "toxicity_control", "empathy", "support"
        ]


    async def run(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        print(f"\nðŸš€ [PersonaGym] Received evaluation request: {input_data}")

 
        purple_agent_url = input_data.get("purple_agent_url") or \
                           input_data.get("url") or \
                           input_data.get("agent_url")

        if not purple_agent_url:
            return {"error": "Missing 'purple_agent_url' in input data."}

        print(f"ðŸŽ¯ Target Purple Agent: {purple_agent_url}")

   
        target_personas = self.personas[:1] 
        
        all_results = []
        for persona in target_personas:
         
            result = await self.evaluate_agent(purple_agent_url, persona, num_questions=1)
            all_results.append(result)

     
        return {
            "status": "success",
            "agent": "PersonaGym Green Agent",
            "timestamp": datetime.now().isoformat(),
            "results": all_results
        }

    def select_environment(self, persona: str) -> str:
        prompt = f"""Given the persona: "{persona}"
Select the most relevant environment from this list:
{json.dumps(self.environments)}
Return only the environment name, nothing else."""
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            env = response.choices[0].message.content.strip()
            return env if env in self.environments else self.environments[0]
        except:
            return self.environments[0]

    def generate_questions(self, persona: str, environment: str, num_questions: int = 5) -> List[Dict[str, str]]:
        prompt = f"""
        Generate {num_questions} mental health scenario questions for:
        Persona: {persona}, Environment: {environment}
        Return ONLY a JSON array of objects with 'question' and 'scenario' fields.
        """
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                response_format={"type": "json_object"}
            )
            content = response.choices[0].message.content
            parsed = json.loads(content)
            questions = parsed.get("questions", parsed.get("data", []))
            if not questions and isinstance(parsed, list): questions = parsed
            return questions[:num_questions]
        except Exception as e:
            print(f"Error generating questions: {e}")
            return [{"question": f"How do you respond as {persona}?", "scenario": environment}]

    async def get_purple_agent_response(self, purple_agent_url: str, persona: str, question: str, scenario: str) -> str:

        return f"[Mock Response] I am playing the role of {persona}. I hear your concern about '{question}'."

    def evaluate_response(self, persona: str, question: str, scenario: str, response: str, dimension: str) -> Dict[str, Any]:
        rubric_text = f"Evaluate {dimension} (0-5) for {persona} in {scenario}."
        prompt = f"""
        {rubric_text}
        Question: {question}
        Response: {response}
        Return JSON: {{ "score": <0-5>, "reasoning": "...", "strengths": "...", "improvements": "..." }}
        """
        try:
            res = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            return json.loads(res.choices[0].message.content)
        except:
            return {"score": 0, "reasoning": "Eval Error"}

    def ensemble_evaluate(self, persona, question, scenario, response, dimension):
        return self.evaluate_response(persona, question, scenario, response, dimension)

    def calculate_persona_score(self, dimension_scores):
        if not dimension_scores: return 0.0
        return round(sum(dimension_scores.values()) / len(dimension_scores), 2)

    async def evaluate_agent(self, purple_agent_url: str, persona: str, num_questions: int = 5) -> Dict[str, Any]:
        print(f"Evaluating {persona}...")
        environment = self.select_environment(persona)
        questions = self.generate_questions(persona, environment, num_questions)
        
        all_evals = []
        for q in questions:
            resp = await self.get_purple_agent_response(purple_agent_url, persona, q['question'], q['scenario'])
            
            dim_evals = {}
            for dim in self.dimensions:
                res = self.ensemble_evaluate(persona, q['question'], q['scenario'], resp, dim)
                dim_evals[dim] = res
            
            all_evals.append({
                "question": q['question'],
                "evaluations": dim_evals
            })

        dim_scores = {dim: 0.0 for dim in self.dimensions}
        if all_evals:
            for dim in self.dimensions:
                scores = [e["evaluations"][dim]["score"] for e in all_evals]
                dim_scores[dim] = sum(scores) / len(scores)

        return {
            "persona": persona,
            "persona_score": self.calculate_persona_score(dim_scores),
            "dimension_scores": dim_scores
        }